{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02374a8d",
   "metadata": {},
   "source": [
    "# üß† KaelumAI Testing Suite\n",
    "\n",
    "**All-in-one testing notebook for development and experimentation**\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup & Configuration](#setup)\n",
    "2. [LLM Selection](#llm-selection) - Choose best model\n",
    "3. [Benchmark Testing](#benchmarks) - GSM8K, TruthfulQA, Speed\n",
    "4. [Verification Testing](#verification) - SymPy, RAG\n",
    "5. [Reflection Testing](#reflection) - Self-improvement\n",
    "6. [Performance Optimization](#performance) - Speed, tokens\n",
    "7. [Integration & Edge Cases](#integration) - Real-world scenarios\n",
    "8. [Experiment Log](#log) - Document findings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a2b1e",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration {#setup}\n",
    "\n",
    "**Configure your testing environment here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaelum import enhance\n",
    "from kaelum.core.config import LLMConfig, MCPConfig\n",
    "from kaelum.core.reasoning import LLMClient\n",
    "from kaelum.core.verification import VerificationEngine\n",
    "import time\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# üéõÔ∏è CONFIGURATION - Change these as needed\n",
    "# ============================================================================\n",
    "\n",
    "# Primary model for testing\n",
    "MODEL = \"llama3.2:3b\"  # Options: llama3.2:3b, qwen2.5:7b, mistral:7b\n",
    "\n",
    "# Test configurations\n",
    "SPEED_MODE = {\"temperature\": 0.3, \"max_tokens\": 512, \"max_iterations\": 1}\n",
    "QUALITY_MODE = {\"temperature\": 0.7, \"max_tokens\": 2048, \"max_iterations\": 2}\n",
    "\n",
    "# Current config\n",
    "CONFIG = SPEED_MODE\n",
    "\n",
    "print(f\"‚úÖ Testing environment ready\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Config: temp={CONFIG['temperature']}, tokens={CONFIG['max_tokens']}, iter={CONFIG['max_iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f70547",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. LLM Selection {#llm-selection}\n",
    "\n",
    "**Choose the best LLM for your use case**\n",
    "\n",
    "### Decision Matrix:\n",
    "\n",
    "| Model | Size | Speed | Quality | Use Case |\n",
    "|-------|------|-------|---------|----------|\n",
    "| **Qwen 2.5 7B** | 4.7GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Production |\n",
    "| **Llama 3.2 3B** | 2.0GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Dev/Fast |\n",
    "| **Mistral 7B** | 4.1GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Code gen |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa62ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models side-by-side\n",
    "MODELS = [\"llama3.2:3b\", \"qwen2.5:7b\"]  # Add more if you have them\n",
    "query = \"What is 15% of 200?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "results = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    print(f\"{'='*60}\\nTesting: {model}\\n{'='*60}\")\n",
    "    start = time.time()\n",
    "    result = enhance(query, model=model, **CONFIG)\n",
    "    elapsed = time.time() - start\n",
    "    results[model] = {\"time\": elapsed, \"result\": result}\n",
    "    print(result)\n",
    "    print(f\"\\n‚è±Ô∏è  {elapsed:.2f}s\\n\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\\nSPEED RANKING\\n{'='*60}\")\n",
    "for model, data in sorted(results.items(), key=lambda x: x[1]['time']):\n",
    "    print(f\"{model:20s} ‚Üí {data['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5754d85",
   "metadata": {},
   "source": [
    "**üìù LLM Selection Notes:**\n",
    "- Fastest model:\n",
    "- Best quality:\n",
    "- Recommended for this project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd3677",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Benchmark Testing {#benchmarks}\n",
    "\n",
    "**Test against project targets (from TODO.md)**\n",
    "\n",
    "### Targets:\n",
    "- Speed: < 500ms overhead\n",
    "- Math (GSM8K): > 90% accuracy\n",
    "- Hallucination (TruthfulQA): > 90% reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84a024",
   "metadata": {},
   "source": [
    "### 3.1 Speed Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed test: baseline vs KaelumAI\n",
    "test_queries = [\"What is 2+2?\", \"What is 25% of 80?\", \"Calculate 15 √ó 7\"]\n",
    "\n",
    "baseline_llm = LLMClient(LLMConfig(model=MODEL))\n",
    "baseline_times = []\n",
    "kaelum_times = []\n",
    "\n",
    "for query in test_queries:\n",
    "    # Baseline\n",
    "    start = time.time()\n",
    "    _ = baseline_llm.generate([{\"role\": \"user\", \"content\": query}])\n",
    "    baseline_times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    # KaelumAI\n",
    "    start = time.time()\n",
    "    _ = enhance(query, model=MODEL, max_iterations=1)\n",
    "    kaelum_times.append((time.time() - start) * 1000)\n",
    "\n",
    "avg_overhead = sum(kaelum_times)/len(kaelum_times) - sum(baseline_times)/len(baseline_times)\n",
    "\n",
    "print(f\"Average baseline: {sum(baseline_times)/len(baseline_times):.0f}ms\")\n",
    "print(f\"Average KaelumAI: {sum(kaelum_times)/len(kaelum_times):.0f}ms\")\n",
    "print(f\"Overhead: {avg_overhead:.0f}ms\")\n",
    "print(f\"\\nTarget: < 500ms\")\n",
    "print(f\"Status: {'‚úÖ PASS' if avg_overhead < 500 else '‚ùå FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d53416",
   "metadata": {},
   "source": [
    "### 3.2 Math Accuracy (GSM8K-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c10be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math problems with known answers\n",
    "math_tests = [\n",
    "    {\"q\": \"If John has 5 apples and buys 3 more, how many?\", \"a\": \"8\"},\n",
    "    {\"q\": \"A $20 shirt is 25% off. What's the sale price?\", \"a\": \"15\"},\n",
    "    {\"q\": \"Solve: 2x + 6 = 14\", \"a\": \"4\"},\n",
    "    {\"q\": \"What is 15% of 200?\", \"a\": \"30\"},\n",
    "    {\"q\": \"Calculate: (12 + 8) √ó 3\", \"a\": \"60\"},\n",
    "]\n",
    "\n",
    "print(\"Testing math accuracy...\\n\")\n",
    "for i, test in enumerate(math_tests, 1):\n",
    "    print(f\"{i}. {test['q']}\")\n",
    "    print(f\"   Expected: {test['a']}\")\n",
    "    result = enhance(test['q'], mode=\"math\", model=MODEL)\n",
    "    print(f\"   Result: {result[:100]}...\\n\")\n",
    "\n",
    "print(\"\\nüëÜ Manually score: correct / {len(math_tests)}\")\n",
    "# TODO: Add your score here\n",
    "# kaelum_correct = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bd04c",
   "metadata": {},
   "source": [
    "### 3.3 Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tricky questions where LLMs hallucinate\n",
    "tricky = [\n",
    "    \"What happens if you crack your knuckles?\",\n",
    "    \"What percentage of the brain do humans use?\",\n",
    "    \"What color is the sky on Mars?\",\n",
    "]\n",
    "\n",
    "for i, q in enumerate(tricky, 1):\n",
    "    print(f\"\\n{'='*60}\\nQ{i}: {q}\\n{'='*60}\")\n",
    "    print(\"\\nBaseline:\")\n",
    "    print(baseline_llm.generate([{\"role\": \"user\", \"content\": q}])[:200])\n",
    "    print(\"\\nKaelumAI:\")\n",
    "    print(enhance(q, model=MODEL, max_iterations=2)[:200])\n",
    "    print(\"\\nüëÜ Did reflection help? Any hallucinations?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab41cc",
   "metadata": {},
   "source": [
    "**üìù Benchmark Results:**\n",
    "- Speed overhead: ___ms (target: <500ms)\n",
    "- Math accuracy: ___% (target: >90%)\n",
    "- Hallucination reduction: Yes/No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88253ccb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Verification Testing {#verification}\n",
    "\n",
    "**Test SymPy symbolic verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7568db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test symbolic verification\n",
    "verifier = VerificationEngine(use_symbolic=True)\n",
    "\n",
    "test_traces = [\n",
    "    {\"name\": \"Correct\", \"trace\": [\"0.25 √ó 80 = 20\"], \"expect\": \"pass\"},\n",
    "    {\"name\": \"Wrong\", \"trace\": [\"0.25 √ó 80 = 25\"], \"expect\": \"fail\"},\n",
    "    {\"name\": \"Equation\", \"trace\": [\"2x + 4 = 10\", \"2x = 6\", \"x = 3\"], \"expect\": \"pass\"},\n",
    "]\n",
    "\n",
    "for test in test_traces:\n",
    "    result = verifier.verify_trace(test['trace'])\n",
    "    status = \"‚úÖ\" if (result.get('valid') and test['expect']=='pass') or (not result.get('valid') and test['expect']=='fail') else \"‚ùå\"\n",
    "    print(f\"{status} {test['name']}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992410a",
   "metadata": {},
   "source": [
    "**üìù Verification Notes:**\n",
    "- SymPy working correctly?\n",
    "- False positives/negatives?\n",
    "- Issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11d37f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Reflection Testing {#reflection}\n",
    "\n",
    "**Does self-reflection improve quality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004612d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with/without reflection\n",
    "complex_query = \"A store has 20% off. A $50 shirt also has a 10% coupon off the sale price. Final price?\"\n",
    "\n",
    "print(\"WITHOUT reflection (iter=1):\\n\" + \"=\"*60)\n",
    "result_no = enhance(complex_query, model=MODEL, max_iterations=1)\n",
    "print(result_no)\n",
    "\n",
    "print(\"\\n\\nWITH reflection (iter=2):\\n\" + \"=\"*60)\n",
    "result_yes = enhance(complex_query, model=MODEL, max_iterations=2)\n",
    "print(result_yes)\n",
    "\n",
    "print(\"\\n\\nCorrect: $36 (20% off $50 = $40, then 10% off $40 = $36)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3ce2a",
   "metadata": {},
   "source": [
    "**üìù Reflection Notes:**\n",
    "- Did reflection improve answer?\n",
    "- Worth the extra time?\n",
    "- Optimal iterations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25bfd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Performance Optimization {#performance}\n",
    "\n",
    "**Identify bottlenecks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f766424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different token limits\n",
    "query = \"Explain why the sky is blue\"\n",
    "token_configs = [256, 512, 1024, 2048]\n",
    "\n",
    "print(\"Testing token limit impact on speed...\\n\")\n",
    "for tokens in token_configs:\n",
    "    start = time.time()\n",
    "    _ = enhance(query, model=MODEL, max_tokens=tokens, max_iterations=1)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    print(f\"max_tokens={tokens:4d} ‚Üí {elapsed:.0f}ms\")\n",
    "\n",
    "print(\"\\nüëÜ Sweet spot for speed vs quality?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test temperature impact\n",
    "temps = [0.0, 0.3, 0.5, 0.7]\n",
    "query = \"Calculate: 25 √ó 8\"\n",
    "\n",
    "print(\"Testing temperature impact...\\n\")\n",
    "for temp in temps:\n",
    "    times = []\n",
    "    for _ in range(2):  # 2 runs\n",
    "        start = time.time()\n",
    "        _ = enhance(query, model=MODEL, temperature=temp, max_iterations=1)\n",
    "        times.append((time.time() - start) * 1000)\n",
    "    print(f\"temp={temp} ‚Üí avg {sum(times)/len(times):.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae19e38",
   "metadata": {},
   "source": [
    "**üìù Performance Notes:**\n",
    "- Bottleneck components:\n",
    "- Optimal max_tokens:\n",
    "- Optimal temperature:\n",
    "- Caching opportunities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df743b55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Integration & Edge Cases {#integration}\n",
    "\n",
    "**Real-world scenarios and error handling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d793cc",
   "metadata": {},
   "source": [
    "### 7.1 Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa447639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test unusual inputs\n",
    "edge_cases = [\n",
    "    \"What is 0 √∑ 0?\",\n",
    "    \"What is infinity + 1?\",\n",
    "    \"What is sqrt(-1)?\",\n",
    "    \"Is this statement false?\",\n",
    "]\n",
    "\n",
    "for query in edge_cases:\n",
    "    print(f\"\\n{'='*60}\\n{query}\\n{'='*60}\")\n",
    "    try:\n",
    "        result = enhance(query, mode=\"math\", model=MODEL)\n",
    "        print(result[:200])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2569f4",
   "metadata": {},
   "source": [
    "### 7.2 Real-World Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Customer support\n",
    "print(\"Scenario 1: Customer Support\\n\" + \"=\"*60)\n",
    "result = enhance(\n",
    "    \"Customer charged twice, order #12345, $99.99. How to refund?\",\n",
    "    mode=\"logic\",\n",
    "    model=MODEL\n",
    ")\n",
    "print(result[:300])\n",
    "\n",
    "# Scenario 2: Educational tutor\n",
    "print(\"\\n\\nScenario 2: Math Tutor\\n\" + \"=\"*60)\n",
    "result = enhance(\n",
    "    \"Explain why (-1) √ó (-1) = 1 for a 10-year-old\",\n",
    "    mode=\"math\",\n",
    "    model=MODEL\n",
    ")\n",
    "print(result[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed0695",
   "metadata": {},
   "source": [
    "**üìù Integration Notes:**\n",
    "- Edge case handling:\n",
    "- Production readiness:\n",
    "- Improvements needed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b369beb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Experiment Log {#log}\n",
    "\n",
    "**Document your findings**\n",
    "\n",
    "### Date: ___________\n",
    "\n",
    "### Model Tested:\n",
    "- \n",
    "\n",
    "### Key Findings:\n",
    "1. **Speed**: \n",
    "2. **Accuracy**: \n",
    "3. **Reflection**: \n",
    "4. **Bottlenecks**: \n",
    "\n",
    "### Pass/Fail Summary:\n",
    "\n",
    "| Test | Target | Result | Status |\n",
    "|------|--------|--------|--------|\n",
    "| Speed | <500ms | ___ms | ___ |\n",
    "| Math | >90% | ___% | ___ |\n",
    "| Hallucination | Reduced | ___ | ___ |\n",
    "| Verification | Working | ___ | ___ |\n",
    "\n",
    "### Bugs Found:\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "### Next Steps:\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "### Team Notes:\n",
    "- Ash:\n",
    "- r3tr0:\n",
    "- wsb:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
