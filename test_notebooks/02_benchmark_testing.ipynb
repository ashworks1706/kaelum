{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ab4c59",
   "metadata": {},
   "source": [
    "# üìä Benchmark Testing\n",
    "\n",
    "**Goal:** Test KaelumAI against standard benchmarks\n",
    "\n",
    "## üéØ 5 Priority Benchmarks (from README):\n",
    "\n",
    "| Benchmark | What it tests | Target |\n",
    "|-----------|---------------|--------|\n",
    "| **Speed** | Latency (ms) | < 500ms overhead |\n",
    "| **Hallucination** | Factual accuracy (TruthfulQA) | > 90% reduction |\n",
    "| **Tool Selection** | Correct tool choice (ToolBench) | > 90% accuracy |\n",
    "| **Math** | Calculation correctness (GSM8K) | > 90% accuracy |\n",
    "| **Orchestration** | Multi-step agent tasks | > 85% success |\n",
    "\n",
    "## üìù Testing Approach:\n",
    "1. Run baseline (LLM without KaelumAI)\n",
    "2. Run with KaelumAI enhancement\n",
    "3. Compare improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaelum import enhance\n",
    "from kaelum.core.reasoning import LLMClient, ReasoningGenerator\n",
    "from kaelum.core.config import LLMConfig\n",
    "import time\n",
    "import json\n",
    "\n",
    "MODEL = \"llama3.2:3b\"  # Change as needed\n",
    "\n",
    "# Setup baseline LLM (without KaelumAI)\n",
    "baseline_llm = LLMClient(LLMConfig(model=MODEL))\n",
    "\n",
    "print(f\"‚úÖ Setup complete for model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9d76e",
   "metadata": {},
   "source": [
    "## Benchmark 1: Speed Test\n",
    "\n",
    "**Target:** < 500ms overhead vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c08bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What is 2+2?\",\n",
    "    \"What is 25% of 80?\",\n",
    "    \"Calculate 15 √ó 7\",\n",
    "    \"What is sqrt(64)?\",\n",
    "    \"Solve: x + 10 = 25\"\n",
    "]\n",
    "\n",
    "baseline_times = []\n",
    "kaelum_times = []\n",
    "\n",
    "print(\"Running speed benchmark...\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Baseline\n",
    "    start = time.time()\n",
    "    _ = baseline_llm.generate([{\"role\": \"user\", \"content\": query}])\n",
    "    baseline_time = (time.time() - start) * 1000\n",
    "    baseline_times.append(baseline_time)\n",
    "    \n",
    "    # With KaelumAI\n",
    "    start = time.time()\n",
    "    _ = enhance(query, model=MODEL, max_iterations=1)\n",
    "    kaelum_time = (time.time() - start) * 1000\n",
    "    kaelum_times.append(kaelum_time)\n",
    "    \n",
    "    overhead = kaelum_time - baseline_time\n",
    "    print(f\"  Baseline: {baseline_time:.0f}ms | KaelumAI: {kaelum_time:.0f}ms | Overhead: {overhead:.0f}ms\\n\")\n",
    "\n",
    "avg_overhead = sum(kaelum_times) / len(kaelum_times) - sum(baseline_times) / len(baseline_times)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Average overhead: {avg_overhead:.0f}ms\")\n",
    "print(f\"Target: < 500ms\")\n",
    "print(f\"Status: {'‚úÖ PASS' if avg_overhead < 500 else '‚ùå FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff0883",
   "metadata": {},
   "source": [
    "**üìù Speed Results:**\n",
    "- Average overhead:\n",
    "- Pass/Fail:\n",
    "- Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16753aac",
   "metadata": {},
   "source": [
    "## Benchmark 2: Math Accuracy (GSM8K-style)\n",
    "\n",
    "**Target:** > 90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample GSM8K-style problems with answers\n",
    "math_problems = [\n",
    "    {\"q\": \"If John has 5 apples and buys 3 more, how many does he have?\", \"a\": \"8\"},\n",
    "    {\"q\": \"A shirt costs $20. If it's 25% off, what's the sale price?\", \"a\": \"15\"},\n",
    "    {\"q\": \"Solve: 2x + 6 = 14\", \"a\": \"4\"},\n",
    "    {\"q\": \"What is 15% of 200?\", \"a\": \"30\"},\n",
    "    {\"q\": \"Calculate: (12 + 8) √ó 3\", \"a\": \"60\"},\n",
    "]\n",
    "\n",
    "baseline_correct = 0\n",
    "kaelum_correct = 0\n",
    "\n",
    "for i, problem in enumerate(math_problems, 1):\n",
    "    print(f\"\\nProblem {i}: {problem['q']}\")\n",
    "    print(f\"Expected: {problem['a']}\")\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_result = baseline_llm.generate([{\"role\": \"user\", \"content\": problem['q']}])\n",
    "    print(f\"Baseline: {baseline_result[:100]}...\")\n",
    "    # TODO: Manual check if correct\n",
    "    \n",
    "    # KaelumAI\n",
    "    kaelum_result = enhance(problem['q'], mode=\"math\", model=MODEL)\n",
    "    print(f\"KaelumAI: {kaelum_result[:200]}...\")\n",
    "    # TODO: Manual check if correct\n",
    "\n",
    "# After manually scoring\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Baseline: {baseline_correct}/{len(math_problems)} ({baseline_correct/len(math_problems)*100:.0f}%)\")\n",
    "print(f\"KaelumAI: {kaelum_correct}/{len(math_problems)} ({kaelum_correct/len(math_problems)*100:.0f}%)\")\n",
    "print(f\"Target: > 90%\")\n",
    "print(f\"Status: {'‚úÖ PASS' if kaelum_correct/len(math_problems) > 0.9 else '‚ùå FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1ab0d",
   "metadata": {},
   "source": [
    "**üìù Math Results:**\n",
    "- KaelumAI accuracy:\n",
    "- Improvement over baseline:\n",
    "- Pass/Fail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe2c29",
   "metadata": {},
   "source": [
    "## Benchmark 3: Hallucination Detection (TruthfulQA-style)\n",
    "\n",
    "**Target:** > 90% reduction in false claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tricky questions where LLMs often hallucinate\n",
    "tricky_questions = [\n",
    "    \"What happens if you crack your knuckles?\",\n",
    "    \"What is the fastest animal on land?\",\n",
    "    \"How long does it take for light from the Sun to reach Earth?\",\n",
    "    \"What percentage of the brain do humans use?\",\n",
    "    \"What color is the sky on Mars?\",\n",
    "]\n",
    "\n",
    "print(\"Testing hallucination detection...\\n\")\n",
    "\n",
    "for i, question in enumerate(tricky_questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\nBaseline:\")\n",
    "    print(baseline_llm.generate([{\"role\": \"user\", \"content\": question}]))\n",
    "    \n",
    "    print(\"\\nKaelumAI (with verification):\")\n",
    "    print(enhance(question, model=MODEL, max_iterations=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342cb5e",
   "metadata": {},
   "source": [
    "**üìù Hallucination Results:**\n",
    "- Manual score baseline:\n",
    "- Manual score KaelumAI:\n",
    "- Improvement:\n",
    "- Did reflection help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79acd3",
   "metadata": {},
   "source": [
    "## Benchmark 4: Logic Reasoning\n",
    "\n",
    "**Test contradiction detection and logical consistency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7370f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_problems = [\n",
    "    \"If all birds can fly, and penguins are birds, can penguins fly?\",\n",
    "    \"John is taller than Mike. Mike is taller than Sarah. Who is shortest?\",\n",
    "    \"A number is even if divisible by 2. Is 15 even or odd?\",\n",
    "]\n",
    "\n",
    "for i, problem in enumerate(logic_problems, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Problem {i}: {problem}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\nBaseline:\")\n",
    "    print(baseline_llm.generate([{\"role\": \"user\", \"content\": problem}]))\n",
    "    \n",
    "    print(\"\\nKaelumAI:\")\n",
    "    print(enhance(problem, mode=\"logic\", model=MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfaa6c4",
   "metadata": {},
   "source": [
    "**üìù Logic Results:**\n",
    "- Did KaelumAI catch contradictions?\n",
    "- Quality improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2121a",
   "metadata": {},
   "source": [
    "## üìä Final Benchmark Summary\n",
    "\n",
    "| Benchmark | Target | Result | Pass/Fail |\n",
    "|-----------|--------|--------|----------|\n",
    "| Speed Overhead | < 500ms | ___ ms | ___ |\n",
    "| Math Accuracy | > 90% | ___% | ___ |\n",
    "| Hallucination | > 90% reduction | ___% | ___ |\n",
    "| Logic Quality | Improved | ___ | ___ |\n",
    "\n",
    "**Next Steps:**\n",
    "1. Document failures\n",
    "2. Optimize slow components\n",
    "3. Add more test cases\n",
    "4. Re-run after improvements"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
