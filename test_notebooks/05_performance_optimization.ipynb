{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c35025",
   "metadata": {},
   "source": [
    "# ‚ö° Performance Optimization Testing\n",
    "\n",
    "**Goal:** Identify bottlenecks and optimize for speed\n",
    "\n",
    "## What We're Testing:\n",
    "1. **Component Latency** - Which parts are slowest?\n",
    "2. **Token Usage** - Optimize prompt efficiency\n",
    "3. **Caching Opportunities** - What to cache?\n",
    "4. **Parallel Processing** - Where can we parallelize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ccbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaelum import enhance\n",
    "from kaelum.runtime.orchestrator import MCP\n",
    "from kaelum.core.config import MCPConfig, LLMConfig\n",
    "import time\n",
    "import functools\n",
    "\n",
    "MODEL = \"llama3.2:3b\"  # Fast model for testing\n",
    "\n",
    "print(f\"‚úÖ Performance testing setup for: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def1556",
   "metadata": {},
   "source": [
    "## Test 1: Component Latency Breakdown\n",
    "\n",
    "**Measure time spent in each pipeline stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85286964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_component(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = (time.time() - start) * 1000\n",
    "        print(f\"{func.__name__}: {elapsed:.0f}ms\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Setup MCP with instrumentation\n",
    "config = MCPConfig(llm=LLMConfig(model=MODEL, max_tokens=512))\n",
    "mcp = MCP(config)\n",
    "\n",
    "# Monkey patch to measure timing\n",
    "original_generate_reasoning = mcp.generator.generate_reasoning\n",
    "original_verify = mcp.verification.verify_trace\n",
    "\n",
    "mcp.generator.generate_reasoning = time_component(original_generate_reasoning)\n",
    "mcp.verification.verify_trace = time_component(original_verify)\n",
    "\n",
    "query = \"What is 15% of 200?\"\n",
    "\n",
    "print(f\"Testing query: {query}\")\n",
    "print(\"\\nComponent timing:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_total = time.time()\n",
    "result = mcp.infer(query)\n",
    "total_time = (time.time() - start_total) * 1000\n",
    "\n",
    "print(f\"\\nTotal: {total_time:.0f}ms\")\n",
    "print(f\"\\nResult: {result['final']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc79f6",
   "metadata": {},
   "source": [
    "**üìù Latency Breakdown:**\n",
    "- Reasoning generation: ___ms\n",
    "- Verification: ___ms\n",
    "- Reflection (if any): ___ms\n",
    "- Bottleneck:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6ea91",
   "metadata": {},
   "source": [
    "## Test 2: Token Usage Analysis\n",
    "\n",
    "**Measure prompt efficiency - fewer tokens = faster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompt configurations\n",
    "test_configs = [\n",
    "    {\"name\": \"Minimal\", \"max_tokens\": 256},\n",
    "    {\"name\": \"Standard\", \"max_tokens\": 512},\n",
    "    {\"name\": \"Detailed\", \"max_tokens\": 1024},\n",
    "]\n",
    "\n",
    "query = \"Solve: 2x + 5 = 15\"\n",
    "\n",
    "for config in test_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config: {config['name']} (max_tokens={config['max_tokens']})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    result = enhance(\n",
    "        query,\n",
    "        mode=\"math\",\n",
    "        model=MODEL,\n",
    "        max_tokens=config['max_tokens'],\n",
    "        max_iterations=1\n",
    "    )\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"Time: {elapsed:.0f}ms\")\n",
    "    print(f\"Response length: {len(result)} chars\")\n",
    "    print(f\"Result preview: {result[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4111ab",
   "metadata": {},
   "source": [
    "**üìù Token Efficiency:**\n",
    "- Sweet spot for max_tokens:\n",
    "- Speed vs quality tradeoff:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa9ad7",
   "metadata": {},
   "source": [
    "## Test 3: Temperature Impact\n",
    "\n",
    "**Lower temperature = faster + more deterministic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f185dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.0, 0.3, 0.5, 0.7]\n",
    "query = \"Calculate: 25 √ó 8\"\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    times = []\n",
    "    for run in range(3):  # 3 runs to average\n",
    "        start = time.time()\n",
    "        result = enhance(query, model=MODEL, temperature=temp, max_iterations=1)\n",
    "        elapsed = (time.time() - start) * 1000\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average time: {avg_time:.0f}ms (across 3 runs)\")\n",
    "    print(f\"Sample result: {result[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9d868",
   "metadata": {},
   "source": [
    "**üìù Temperature Results:**\n",
    "- Fastest temperature:\n",
    "- Quality impact:\n",
    "- Recommendation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db14297",
   "metadata": {},
   "source": [
    "## Test 4: Batch Processing\n",
    "\n",
    "**Test multiple queries - identify caching opportunities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar queries that could benefit from caching\n",
    "queries = [\n",
    "    \"What is 10% of 100?\",\n",
    "    \"What is 20% of 100?\",\n",
    "    \"What is 30% of 100?\",\n",
    "    \"What is 10% of 100?\",  # Duplicate - should be cached\n",
    "]\n",
    "\n",
    "print(\"Testing batch queries (cache=True)...\\n\")\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    start = time.time()\n",
    "    result = enhance(query, model=MODEL, cache=True, max_iterations=1)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    print(f\"Time: {elapsed:.0f}ms\")\n",
    "    \n",
    "    if i == 4:  # Should be faster (cache hit)\n",
    "        print(\"üëÜ This should be instant (cache hit)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69e28d",
   "metadata": {},
   "source": [
    "**üìù Caching Analysis:**\n",
    "- Did duplicate query use cache?\n",
    "- Speed improvement:\n",
    "- What else to cache?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401effb",
   "metadata": {},
   "source": [
    "## Test 5: Skip Reflection for Simple Queries\n",
    "\n",
    "**Test confidence threshold tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_queries = [\n",
    "    \"What is 5 + 5?\",\n",
    "    \"What is 10 √ó 2?\",\n",
    "    \"What is 100 √∑ 4?\",\n",
    "]\n",
    "\n",
    "print(\"Testing simple queries (should skip reflection)...\\n\")\n",
    "\n",
    "for query in simple_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    start = time.time()\n",
    "    result = enhance(query, model=MODEL, max_iterations=2)  # Allow reflection\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    # Check if reflection was skipped\n",
    "    import re\n",
    "    iterations = 0\n",
    "    match = re.search(r'iterations?:\\s*(\\d+)', result.lower())\n",
    "    if match:\n",
    "        iterations = int(match.group(1))\n",
    "    \n",
    "    print(f\"Time: {elapsed:.0f}ms\")\n",
    "    print(f\"Iterations: {iterations} (0 = skipped reflection ‚úÖ)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d740c",
   "metadata": {},
   "source": [
    "**üìù Reflection Optimization:**\n",
    "- Are simple queries fast?\n",
    "- Reflection skipped correctly?\n",
    "- Threshold tuning needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8153f",
   "metadata": {},
   "source": [
    "## üéØ Performance Summary\n",
    "\n",
    "| Optimization | Current | Target | Status |\n",
    "|--------------|---------|--------|--------|\n",
    "| Reasoning latency | ___ms | < 2000ms | ___ |\n",
    "| Verification overhead | ___ms | < 100ms | ___ |\n",
    "| Total pipeline | ___ms | < 3000ms | ___ |\n",
    "| Token efficiency | ___ tokens | < 1000 | ___ |\n",
    "\n",
    "**Bottlenecks Found:**\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "**Optimization Recommendations:**\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement recommended optimizations\n",
    "- Add LRU caching for common queries\n",
    "- Consider async/parallel processing\n",
    "- Profile with production data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
