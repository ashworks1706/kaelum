{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b5a066",
   "metadata": {},
   "source": [
    "# üîÑ Reflection Engine Testing\n",
    "\n",
    "**Goal:** Test self-reflection and iterative improvement\n",
    "\n",
    "## What We're Testing:\n",
    "1. **Reflection Triggers** - When does it reflect?\n",
    "2. **Improvement Quality** - Does reflection actually help?\n",
    "3. **Iteration Count** - Optimal number of reflection passes\n",
    "4. **Confidence Scoring** - Are confidence scores accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d669bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaelum import enhance\n",
    "from kaelum.core.reflection import ReflectionEngine\n",
    "from kaelum.core.reasoning import LLMClient\n",
    "from kaelum.core.config import LLMConfig\n",
    "import re\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "llm = LLMClient(LLMConfig(model=MODEL))\n",
    "reflection_engine = ReflectionEngine(llm, max_iterations=3)\n",
    "\n",
    "print(f\"‚úÖ Reflection engine loaded with model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274eada",
   "metadata": {},
   "source": [
    "## Test 1: Reflection Trigger Conditions\n",
    "\n",
    "**Test what triggers reflection vs direct answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with varying complexity\n",
    "test_queries = [\n",
    "    {\"q\": \"What is 2+2?\", \"expected_reflection\": False},\n",
    "    {\"q\": \"What is 25% of 80?\", \"expected_reflection\": False},\n",
    "    {\"q\": \"If all birds fly and penguins are birds, can penguins fly?\", \"expected_reflection\": True},\n",
    "    {\"q\": \"Solve: 3x^2 + 5x - 2 = 0\", \"expected_reflection\": True},\n",
    "]\n",
    "\n",
    "for test in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {test['q']}\")\n",
    "    print(f\"Expected reflection: {test['expected_reflection']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = enhance(test['q'], model=MODEL, max_iterations=2)\n",
    "    \n",
    "    # Check if reflection occurred (look for iteration count in output)\n",
    "    iterations = 0\n",
    "    if \"iterations\" in result.lower():\n",
    "        match = re.search(r'iterations?:\\s*(\\d+)', result.lower())\n",
    "        if match:\n",
    "            iterations = int(match.group(1))\n",
    "    \n",
    "    reflected = iterations > 0\n",
    "    status = \"‚úÖ CORRECT\" if reflected == test['expected_reflection'] else \"‚ö†Ô∏è  UNEXPECTED\"\n",
    "    \n",
    "    print(f\"\\nActual iterations: {iterations}\")\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"\\nResult preview: {result[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36818df7",
   "metadata": {},
   "source": [
    "**üìù Trigger Results:**\n",
    "- Are triggers appropriate?\n",
    "- Too many false triggers?\n",
    "- Too few reflections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07bed8",
   "metadata": {},
   "source": [
    "## Test 2: Quality Improvement via Reflection\n",
    "\n",
    "**Does reflection actually improve answers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex query that benefits from reflection\n",
    "query = \"\"\"A store has a sale: 20% off all items. If a shirt originally costs $50,\n",
    "and you have a coupon for an additional 10% off the sale price, what is the final price?\"\"\"\n",
    "\n",
    "print(\"Test WITHOUT reflection (max_iterations=1):\")\n",
    "print(\"=\"*60)\n",
    "result_no_reflection = enhance(query, model=MODEL, max_iterations=1, temperature=0.3)\n",
    "print(result_no_reflection)\n",
    "\n",
    "print(\"\\n\\nTest WITH reflection (max_iterations=2):\")\n",
    "print(\"=\"*60)\n",
    "result_with_reflection = enhance(query, model=MODEL, max_iterations=2, temperature=0.3)\n",
    "print(result_with_reflection)\n",
    "\n",
    "print(\"\\n\\nüëÜ Compare the two answers above\")\n",
    "print(\"Correct answer: $36 (20% off $50 = $40, then 10% off $40 = $36)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f1e81",
   "metadata": {},
   "source": [
    "**üìù Quality Comparison:**\n",
    "- Without reflection:\n",
    "- With reflection:\n",
    "- Improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977886b",
   "metadata": {},
   "source": [
    "## Test 3: Optimal Iteration Count\n",
    "\n",
    "**Test 1, 2, 3 iterations - when does it stop helping?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ef9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query = \"\"\"If a car travels 60 km in 45 minutes, and then 90 km in 1.5 hours,\n",
    "what is the average speed in km/h?\"\"\"\n",
    "\n",
    "for iterations in [1, 2, 3]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with max_iterations={iterations}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    result = enhance(query, mode=\"math\", model=MODEL, max_iterations=iterations)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(result)\n",
    "    print(f\"\\n‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(\"\\n\\nCorrect answer: 66.67 km/h (150 km / 2.25 hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02f0bd",
   "metadata": {},
   "source": [
    "**üìù Iteration Analysis:**\n",
    "- 1 iteration:\n",
    "- 2 iterations:\n",
    "- 3 iterations:\n",
    "- Optimal count:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75044436",
   "metadata": {},
   "source": [
    "## Test 4: Confidence Score Accuracy\n",
    "\n",
    "**Are confidence scores meaningful?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with known difficulty levels\n",
    "confidence_tests = [\n",
    "    {\"q\": \"What is 5 + 3?\", \"difficulty\": \"easy\"},\n",
    "    {\"q\": \"What is 17% of 250?\", \"difficulty\": \"medium\"},\n",
    "    {\"q\": \"Solve: log(x) + log(x-3) = 1\", \"difficulty\": \"hard\"},\n",
    "]\n",
    "\n",
    "for test in confidence_tests:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query ({test['difficulty']}): {test['q']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = enhance(test['q'], mode=\"math\", model=MODEL)\n",
    "    \n",
    "    # Extract confidence if shown\n",
    "    confidence = None\n",
    "    match = re.search(r'confidence:\\s*(\\d+)%', result.lower())\n",
    "    if match:\n",
    "        confidence = int(match.group(1))\n",
    "    \n",
    "    print(result)\n",
    "    print(f\"\\nExtracted confidence: {confidence}%\")\n",
    "    print(f\"Expected: Higher for easier questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7c61a",
   "metadata": {},
   "source": [
    "**üìù Confidence Scores:**\n",
    "- Do they correlate with difficulty?\n",
    "- Are they calibrated?\n",
    "- Issues found:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18835318",
   "metadata": {},
   "source": [
    "## üéØ Reflection Summary\n",
    "\n",
    "| Aspect | Finding | Recommendation |\n",
    "|--------|---------|----------------|\n",
    "| Trigger Logic | ___ | ___ |\n",
    "| Quality Improvement | ___ | ___ |\n",
    "| Optimal Iterations | ___ | ___ |\n",
    "| Confidence Scores | ___ | ___ |\n",
    "\n",
    "**Next Steps:**\n",
    "1. Tune confidence thresholds\n",
    "2. Optimize iteration logic\n",
    "3. Improve reflection prompts\n",
    "4. Add early stopping criteria"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
