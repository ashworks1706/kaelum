
# ğŸ§  KaelumAI

**Reasoning Acceleration Layer for Lightweight LLMs**

> ğŸ§ª **Testing:** One comprehensive notebook in `test_notebooks/testing.ipynb` - covers LLM selection, benchmarks, verification, reflection, performance, and integration testing

---

## ğŸŒ Overview

**KaelumAI** is a *modular reasoning verification layer* designed to make small and mid-sized LLMs **think better, faster, and more reliably** â€” without costly finetuning.

It acts as a **middleware MCP (Model-Context Protocol)** between your application and the base model, enabling contextual reasoning, symbolic math verification, factual guardrails, and adaptive reflection loops â€” all with minimal latency and cost.

---

## ğŸ’¡ The Problem

Smaller and cheaper LLMs (e.g. **Llama 3 3B**, **Mistral 7B**, **Gemini Flash**) are fast but unreliable:

* âŒ Poor reasoning and logical consistency
* âŒ Frequent hallucinations
* âŒ Wrong tool selection
* âŒ Weak math performance
* âŒ Inefficient agent orchestration

Traditional fixes like **RLHF**, **fine-tuning**, or **distillation** are expensive and slow.

---

## âš™ï¸ Our Solution â€” *Inference-Time Reasoning Enhancement*

KaelumAI adds a verification and reflection layer *at inference*, not training:

| Layer                 | Description                                                               |
| --------------------- | ------------------------------------------------------------------------- |
| ğŸ§© **Contextualizer** | Builds structured reasoning context from history, RAG, and tools          |
| ğŸ” **Verifier**       | Performs symbolic, factual, and numeric checks using deterministic rules  |
| ğŸ”„ **Reflexor**       | Runs lightweight self-reflection passes to correct low-confidence outputs |
| ğŸ§  **Orchestrator**   | Routes to the right tool/agent dynamically using cost-aware policies      |
| ğŸ“Š **Tracer**         | Produces transparent reasoning traces for debugging and interpretability  |

**Goals:**

* âš¡ **Speed:** < 500 ms overhead
* ğŸ’¸ **Efficiency:** Single LLM, smart caching
* ğŸ¯ **Accuracy:** > 90 % gain on reasoning benchmarks

---

## ğŸš€ Quick Start

```bash
# Clone and install
git clone https://github.com/ashworks1706/KaelumAI.git
cd KaelumAI
pip install -r requirements.txt

# Pull a local model via Ollama
ollama pull qwen2.5:7b
```

```python
from kaelum import enhance

# Simple usage
result = enhance("What is 25% of 80?")

# Customize parameters
result = enhance(
    "Explain quantum entanglement",
    model="qwen2.5:7b",
    temperature=0.7,
    max_tokens=2048,
    max_iterations=2,
)
```

**ğŸ§ª For Testing & Experiments:**

Open `test_notebooks/testing.ipynb` in Jupyter:
- All-in-one testing suite with 8 organized sections
- Pre-configured test cells for different scenarios
- Speed vs Quality mode comparisons
- Model benchmarking (llama3.2:3b vs qwen2.5:7b)
- Document findings inline with markdown
- Sequential testing workflow for fast iteration

**âš¡ Quick Demo:**
```bash
python example.py  # Simple one-shot demo
```

---

**âš¡ Speed vs Quality Trade-offs:**

| Mode | temperature | max_tokens | max_iterations | Speed | Quality |
|------|-------------|------------|----------------|-------|---------|
| **Speed** | 0.3 | 512 | 1 | âš¡âš¡âš¡ Fast (2-3s) | â­â­ Good |
| **Balanced** | 0.5 | 1024 | 1 | âš¡âš¡ Medium (4-6s) | â­â­â­ Better |
| **Quality** | 0.7 | 2048 | 2 | âš¡ Slow (8-12s) | â­â­â­â­ Best |

**ğŸ¯ Quick Start:**
```bash
# Default (llama3.2:3b, speed mode)
python example.py

# Specify model
python example.py qwen2.5:7b

# Specify model + mode
python example.py llama3.2:3b balanced
python example.py qwen2.5:7b quality

# Or edit presets directly in example.py
```

---

## ğŸ“ Project Structure

```
kaelum/
â”œâ”€â”€ __init__.py             # Public API: enhance() function
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py          # Settings & environment config
â”‚   â”œâ”€â”€ reasoning.py       # LLM client & trace generation
â”‚   â”œâ”€â”€ verification.py    # SymPy symbolic math verifier
â”‚   â”œâ”€â”€ reflection.py      # Self-reflection loop
â”‚   â””â”€â”€ rag_adapter.py     # RAG connectors (ChromaDB, Qdrant)
â””â”€â”€ runtime/
    â””â”€â”€ orchestrator.py    # MCP pipeline coordinator

test_notebooks/              # ğŸ§ª Complete testing suite
â”œâ”€â”€ 01_llm_selection.ipynb         # Choose best LLM
â”œâ”€â”€ 02_benchmark_testing.ipynb     # GSM8K, TruthfulQA, ToolBench
â”œâ”€â”€ 03_verification_testing.ipynb  # SymPy + RAG testing
â”œâ”€â”€ 04_reflection_testing.ipynb    # Self-improvement testing
â”œâ”€â”€ 05_performance_optimization.ipynb  # Speed optimization
â””â”€â”€ 06_integration_edge_cases.ipynb    # Real-world scenarios
```

**Key Files:**
- `kaelum/__init__.py` â†’ Main API entry point
- `reasoning.py` â†’ Handles LLM calls & reasoning trace generation
- `verification.py` â†’ Verifies math/logic using SymPy
- `orchestrator.py` â†’ Runs verification â†’ reflection loop
- `test_notebooks/` â†’ **Start here for testing and development**
- `example.py` â†’ Quick demo (single query)

---

## ğŸ§© Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        User Query          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      Context Builder
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Base LLM (e.g. 7B)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
   Reasoning Trace & Output
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Verification Layer (Kaelum)â”‚
â”‚ - Symbolic check (SymPy)   â”‚
â”‚ - Factual check (RAG)      â”‚
â”‚ - Self-reflection loop     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
      Enhanced & Verified Response
```

---

## ğŸ§± Development Roadmap

### ğŸ— Sprint 1 â€” Core MVP

* [ ] LLM client (Ollama, OpenAI, vLLM)
* [ ] Reasoning trace generation
* [ ] Symbolic verification (SymPy)
* [ ] Confidence scoring
* [ ] One-line API

### ğŸ” Sprint 2 â€” Verification Layer

* [ ] RAG adapters (ChromaDB, Qdrant)
* [ ] Factual verification layer
* [ ] Self-reflection loop
* [ ] Adaptive stopping

### âš¡ Sprint 3 â€” Optimization

* [ ] LRU + Redis caching
* [ ] Tool selection guardrails
* [ ] Agent orchestration
* [ ] Prompt optimization

### ğŸ§  Sprint 4 â€” Benchmarks & Testing

* [ ] Speed benchmarks
* [ ] Hallucination detection tests
* [ ] Tool selection accuracy
* [ ] Math reasoning tests
* [ ] Agent orchestration tests

---

## ğŸ“Š Benchmark Decision Matrix

| Use Case                    | Recommended Benchmarks                   | Why                                   |
| --------------------------- | ---------------------------------------- | ------------------------------------- |
| **Production API**          | Speed + Cost Analysis                    | Evaluate scalability and latency      |
| **Customer Support Bot**    | Hallucination Detection + Tool Selection | Needs reliable factual grounding      |
| **Math Tutor / STEM Agent** | GSM8K / MATH + HalluEval                 | Symbolic & numerical correctness      |
| **Research Assistant**      | TruthfulQA + ToolBench                   | Factual precision + API correctness   |
| **Code Assistant**          | ToolBench + Math Reasoning               | Logic + algorithmic reliability       |
| **Multi-Agent System**      | Agent Orchestration Benchmark            | Proper task routing and collaboration |
| **General Purpose**         | All five                                 | Comprehensive evaluation              |

**Available Benchmarks**

* âš¡ Speed â€” Latency / token throughput
* ğŸ§  Math Reasoning â€” GSM8K / MATH subset
* ğŸ” Hallucination â€” TruthfulQA / HalluEval
* ğŸ§° Tool Selection â€” ToolBench subset
* ğŸ¤ Agent Orchestration â€” Custom workflow tests

---

## ğŸ§® LLM Decision Matrix (2025 Update)

| Constraint                | Recommended Models                                        | Why                                           |
| ------------------------- | --------------------------------------------------------- | --------------------------------------------- |
| **Local / Privacy-First** | **Qwen 2.5 7B**, **Llama 3.2 3B**, **Mistral 7B**         | Fully local via Ollama / vLLM                 |
| **Best Overall Quality**  | **Gemma 2 9B**, **Qwen 2.5 14B**, **Llama 3.1 8B**        | High reasoning + factual scores               |
| **Fastest (Edge / CPU)**  | **Phi-3 Mini (3.8B)**, **Llama 3.2 3B**                   | Sub-second inference on 8 GB GPU              |
| **Math-Heavy Reasoning**  | **Qwen 2.5 7B**, **DeepSeek Math 7B**, **DeepSeek R1 8B** | Specialized math training datasets            |
| **Long Context / Memory** | **Llama 3.2 3B (128K)**, **Gemma 2 9B (32K)**             | Extended context for multi-agent coordination |
| **Low VRAM Deployment**   | **Phi-3 Mini**, **Llama 3.2 3B**                          | Fits on laptops / 8 GB GPUs                   |
| **Balanced All-Rounder**  | **Mistral 7B**, **Qwen 2.5 7B**, **Llama 3.1 8B**         | Great mix of cost / latency / accuracy        |

**Model Specs (4-bit Quantized Reference)**

| Model            | Size | Context | VRAM (4-bit) | Speed | Notes                         |
| ---------------- | ---- | ------- | ------------ | ----- | ----------------------------- |
| Llama 3.2 3B     | 3B   | 128K    | â‰ˆ 2.5 GB     | âš¡âš¡âš¡   | Fastest baseline for Kaelum   |
| Llama 3.1 8B     | 8B   | 128K    | â‰ˆ 5 GB       | âš¡âš¡    | Balanced quality/speed        |
| Qwen 2.5 7B      | 7B   | 32K     | â‰ˆ 4.5 GB     | âš¡âš¡    | Strong in math & code         |
| DeepSeek R1 8B   | 8B   | 16K     | â‰ˆ 5 GB       | âš¡âš¡    | Reasoning-optimized           |
| DeepSeek Math 7B | 7B   | 16K     | â‰ˆ 4.5 GB     | âš¡âš¡    | Symbolic math expert          |
| Mistral 7B       | 7B   | 32K     | â‰ˆ 4.5 GB     | âš¡âš¡    | General purpose               |
| Phi-3 Mini 3.8B  | 3.8B | 128K    | â‰ˆ 2.8 GB     | âš¡âš¡âš¡   | Ultra-fast edge model         |
| Gemma 2 9B       | 9B   | 32K     | â‰ˆ 5.5 GB     | âš¡     | High quality from Google      |
| Qwen 2.5 14B     | 14B  | 32K     | â‰ˆ 8 GB       | âš¡     | Top open reasoning model 2025 |

âš¡ = speed rating (fewer âš¡ â†’ slower but smarter)

---

## ğŸ¯ Target Metrics

| Priority         | Metric                       | Target   |
| ---------------- | ---------------------------- | -------- |
| âš¡ Speed          | Latency Overhead             | < 500 ms |
| ğŸ§  Reasoning     | Math Correctness             | > 95 %   |
| ğŸ” Factuality    | Hallucination Detection Rate | > 90 %   |
| ğŸ§° Tool Use      | Correct Tool Selection       | > 85 %   |
| ğŸ¤– Orchestration | Agent Accuracy               | > 80 %   |
| ğŸ’¸ Cost          | $/1K queries                 | < $0.10  |

---

## ğŸ“š Suggested Research & References

* [Anthropic â€“ *Tracing the Thoughts of a Language Model* (2025)](https://www.anthropic.com/research/tracing-thoughts-language-model)
* [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)
* [Self-RAG (2024): Retrieval-Augmented Generation with Self-Verification](https://arxiv.org/abs/2310.06112)
* [DeepSeek R1 Technical Report (2025)](https://medium.com/data-science-in-your-pocket/deepseek-r1-best-open-source-reasoning-llm-outperforms-openai-o1-b79869392945)
* [ToolBench: Benchmarking LLM Tool Use and APIs](https://github.com/openbmb/toolbench)
* [TruthfulQA / HalluEval for Hallucination Testing](https://github.com/sylinrl/hallueval)
* [GSM8K / MATH Datasets for Reasoning](https://github.com/openai/grade-school-math)
