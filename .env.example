# Kaelum AI Environment Configuration
# Copy this file to .env and configure for your setup

# ============================================================================
# vLLM Server Configuration (Local Reasoning Engine)
# ============================================================================
VLLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v0.3
VLLM_PORT=8000
VLLM_GPU_MEMORY_UTILIZATION=0.7
VLLM_MAX_MODEL_LEN=1024
VLLM_MAX_NUM_SEQS=32

# ============================================================================
# Kaelum Core Configuration
# ============================================================================
KAELUM_REASONING_URL=http://localhost:8000/v1
KAELUM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v0.3
KAELUM_TEMPERATURE=0.7
KAELUM_MAX_TOKENS=2048
KAELUM_MAX_REFLECTION_ITERATIONS=2

# ============================================================================
# Verification Settings
# ============================================================================
KAELUM_USE_SYMBOLIC_VERIFICATION=true
KAELUM_USE_FACTUAL_VERIFICATION=false
KAELUM_DEBUG_VERIFICATION=false

# ============================================================================
# Commercial LLM API Keys (Optional - for agent integration)
# ============================================================================
OPENAI_API_KEY=your-openai-key-here
GOOGLE_API_KEY=your-google-key-here
GEMINI_API_KEY=your-gemini-key-here
ANTHROPIC_API_KEY=your-anthropic-key-here

# ============================================================================
# Logging
# ============================================================================
LOG_LEVEL=INFO
KAELUM_LOG_FILE=./logs/kaelum.log
ENVIRONMENT=production

# ============================================================================
# Performance & Caching
# ============================================================================
KAELUM_ENABLE_CACHING=true
KAELUM_CACHE_TTL=3600

# ============================================================================
# Recommended Models (uncomment to use)
# ============================================================================

# Qwen 1.5B (Best balance - recommended for production)
# VLLM_MODEL=Qwen/Qwen2.5-1.5B-Instruct
# KAELUM_MODEL=Qwen/Qwen2.5-1.5B-Instruct

# Qwen 7B (Best accuracy - requires 8GB+ VRAM)
# VLLM_MODEL=Qwen/Qwen2.5-7B-Instruct
# KAELUM_MODEL=Qwen/Qwen2.5-7B-Instruct

# Phi-3 Mini (Efficient - good for 6GB VRAM)
# VLLM_MODEL=microsoft/Phi-3-mini-4k-instruct
# KAELUM_MODEL=microsoft/Phi-3-mini-4k-instruct

# Mistral 7B (Balanced - good general reasoning)
# VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.3
# KAELUM_MODEL=mistralai/Mistral-7B-Instruct-v0.3
